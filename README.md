# sc-api-challenge

### Table of Contents

1. [Project Setup](#1-project-setup)
1. [Running Tests Locally](#2-running-tests-locally)
1. [Running tests with TravisCI](#3-runing-tests-with-travis-ci)
1. [Calliope.pro test results](#4-calliope.pro-test-results)
1. [Picking Scenarios to Automate](#5-picking-scenarios-to-automate)
1. [Next Steps](#6-next-steps)

### 1. Project Setup

**Description:** Assuming that you have [NodeJS](https://nodejs.org/es/download/package-manager/) installed you should only need to clone the repo to your machine and run

    ```
    npm install
    ```

That will install all the required dependencies for the framework to function.

### 2. Running Tests Locally

To run the tests locally you only need to execute the following command: 

    ```
    npm test
    ```

### 3. Running Tests with TravisCI
This repo has [TravisCI Integrration](https://app.travis-ci.com/github/cmpinzonh/sc-api-challenge) which means that with every push to a remote branch a build will be triggered, that including attempts to merge to a pull request. Manually triggering a build of a specific desired branch is also possible.

### 4. Calliope.pro Report
I created a sandbox company in Calliope.pro and updated the report generated by [mochawesome](https://www.npmjs.com/package/mochawesome), said report can be found in: [Calliope.pro report](https://app.calliope.pro/reports/106212/public/3c05b755-5dfd-4319-9596-57179b74d66f).

### 5. Picking Scenarios to Automate
1. Add new pet.
Considering the main focus of the application relies on the pet inventory I decided to start by testing the Add new pet feature, identified what fields are required to successfully add a pet to the inventory and created a test scenaraio where I stablish the desired Id for the pet, that way I can reliably query for that Id with full knowledge that it exists within the application. 
1. Get existing pet.
Following the same train of thought I decided I wanted to verify that, once I had introduced a new Pet into the application, the system was able to query for it and successfully display the information of the pet as I had created it.
1. Query for non existent pet.
Since it's likeyly that a customer might query for pets that have not been created on the application I decided to test the baheviour of the API when asked to look for a non-existent pet, in this case the API responds with Status Code 404 (which is standard but not certain, specially considering that the Create Pet API responds 200 OK on a successfull request intead of 201 created) and contains a small error message that can also be verified within the test (which I do).
It's tough to make decision on what features of an application to test having close to non information, while from my point of view these may be some of the key features of the application there's a scenario in which a stakeholder with a lot of knowledge of the business considers a different part of the application to be the most worthy of testing (say, user creation and log in) which makes it hard to assert whether or not I made the right choice when picking my scenarios to automate.

### 6. Next Steps
More information would be required to know where to expand the testing effort, the application seems to have a users module with authentication included but it is not required to interact with the pets API which raises questions regarding how important it is to the business. We could also look into leveraging the quite well document API to generate DTOs for every entity in the model, which would speed up the testing effort and bring more clarity regarding the rules and limitations of the system, which would then help more clearly identify key features that require or would be deserving of more testing. 
